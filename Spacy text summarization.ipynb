{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f18ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44870068",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"data science and ai has greater career ahead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0eb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"Apple is looking at buying U.K. startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,token.lemma_,token.pos_,token.tag_,token.dep_,token.shape_,token.is_alpha,token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19419162",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\\nAn example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\\nImage collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[4] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6995aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f81269",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc585e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c34211",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ba65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3cf69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import spacy\n",
    "#!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d085ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr=spacy.load('fr_core_news_sm')\n",
    "nlp_en=spacy.load('en_core_web_sm')\n",
    "text_1=\"L'intelligence artificielle (IA) fait référence au développement de systèmes informatiques ou de machines capables d'effectuer des tâches qui nécessitent généralement l'intelligence humaine.\"\n",
    "doc=nlp_fr(text_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b7ef0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L'intelligence artificielle (IA) fait référence au développement de systèmes informatiques ou de machines capables d'effectuer des tâches qui nécessitent généralement l'intelligence humaine."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efe1e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_text=doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad598730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L'intelligence artificielle (IA) fait référence au développement de systèmes informatiques ou de machines capables d'effectuer des tâches qui nécessitent généralement l'intelligence humaine."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd894ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_fr=nlp_fr(french_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e0c534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text=\" \".join(token.text for token in doc_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbcab997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"L' intelligence artificielle ( IA ) fait référence au développement de systèmes informatiques ou de machines capables d' effectuer des tâches qui nécessitent généralement l' intelligence humaine .\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f17da214",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m translator\u001b[38;5;241m=\u001b[39mTranslator()\n\u001b[1;32m----> 2\u001b[0m converted_text\u001b[38;5;241m=\u001b[39mtranslator\u001b[38;5;241m.\u001b[39mtranslate(processed_text,dest\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m translation\u001b[38;5;241m=\u001b[39mconverted_text\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32m~\\anaconda3\\anaconda\\Lib\\site-packages\\googletrans\\client.py:182\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[1;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    181\u001b[0m origin \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m--> 182\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_translate(text, dest, src, kwargs)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# this code will be updated when the format is changed.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[1;32m~\\anaconda3\\anaconda\\Lib\\site-packages\\googletrans\\client.py:78\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[1;34m(self, text, dest, src, override)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_translate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, dest, src, override):\n\u001b[1;32m---> 78\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_acquirer\u001b[38;5;241m.\u001b[39mdo(text)\n\u001b[0;32m     79\u001b[0m     params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[0;32m     80\u001b[0m                                 token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[0;32m     82\u001b[0m     url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n",
      "File \u001b[1;32m~\\anaconda3\\anaconda\\Lib\\site-packages\\googletrans\\gtoken.py:194\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update()\n\u001b[0;32m    195\u001b[0m     tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire(text)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "File \u001b[1;32m~\\anaconda3\\anaconda\\Lib\\site-packages\\googletrans\\gtoken.py:62\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRE_TKK\u001b[38;5;241m.\u001b[39msearch(r\u001b[38;5;241m.\u001b[39mtext)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode-escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "translator=Translator()\n",
    "converted_text=translator.translate(processed_text,dest='en')\n",
    "translation=converted_text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb302fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34220b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c21b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies={}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text]=1\n",
    "            else:\n",
    "                 word_frequencies[word.text]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    " word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq=max(word_frequencies.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a12410",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word]=word_frequencies[word]/max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f004db",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e022f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens=[sent for sent in doc.sents]\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c311b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "\n",
    "for sent in sent_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_scores.keys():\n",
    "                sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "                sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eddb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_length= int(len(sent_tokens)*0.4)\n",
    "select_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99310785",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=nlargest(select_length,sentence_scores,key=sentence_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71e1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
